

**Вопрос 1:**  
Атаки уклонения направлены на нарушение доступности.

**Вопрос 2:**  
При атаках «черного ящика» злоумышленнику может потребоваться создать собственную суррогатную модель, имитирующую целевую модель.

**Вопрос 3:**  
Атака «белого ящика» применяется при знании злоумышленником полной информации о модели МО.

**Вопрос 4:**  
Атаки «белого» и «черного ящика» относятся к атакам на этапе тестирования и эксплуатации.

**Вопрос 5:**  
Малое применение систем на основе ИИ в областях с высокими рисками связано с тем, что модели, обученные на некоторых наборах данных, в реальной среде могут давать большое количество ошибок и сбоев.

**Вопрос 6:**  
Чтобы обмануть нейронную сеть, распознающую человека в видеопотоке, можно использовать: маску со специальным принтом, разноцветные полосы на одежде, специальную светоотражающую косметику, очки со специальным принтом, правильно подобранный цвет одежды, или распечатанное изображение, нанесенное на одежду.

**Вопрос 7:**  
В современном ИИ выделяют направления: слабый ИИ и сильный ИИ.

**Вопрос 8:**  
Слабые ИИ разработаны и обучены для выполнения конкретной задачи, а сильные ИИ способны решать любую незнакомую интеллектуальную задачу, которую способен решить человек.

**Вопрос 9:**  
При обучении с подкреплением наборы данных не размечаются, но система получает соответствующую обратную связь.

**Вопрос 10:**  
Обучение без учителя учится группировать схожие по определенным признакам события или объекты.

**Вопрос 11:**  
Проблема недообучения модели заключается в том, что модель хорошо адаптируется к обучающей выборке, но не способна к обобщению новых данных.

**Вопрос 12:**  
К слабостям систем, основанных на ИИ, относятся: неправильный выбор тестовых данных, сравнительно легко получить «теневую» или суррогатную модель за счет изучения защищаемой модели как «черного ящика», а также обучающая выборка не может отражать всех данных генеральной совокупности.

**Вопрос 13:**  
Отравляющие атаки относятся к атакам на этапе обучения.

**Вопрос 14:**  
При атаках уклонения злоумышленник пытается обойти систему путем корректировки состязательных данных во время фазы тестирования.

**Вопрос 15:**  
Состязательные атаки, когда состязательные примеры вносят в набор обучающих данных, относятся к отравляющим атакам.

**Вопрос 16:**  
В атаке уклонения злоумышленнику известны результаты работы целевой модели, у него есть возможность создать и использовать сгенерированные состязательные примеры для нарушения работы системы, но он не влияет на данные, используемые для обучения целевой модели.


